{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinkful Data Science Final Capstone\n",
    "## News Article Summarization Using Keras RNNs\n",
    "\n",
    "Facing an increasingly polarized readership, news publications commonly resort to biased journalism along a sociopolitical spectrum of left or right leaning subjectivity. Websites like Vox and Breitbart News cover many of the same stories, but from entirely different perspectives intended to enhance the confirmation bias of the reader, a proven customer retention strategy. \n",
    "\n",
    "A recently created website, [Knowhere](http://www.knowhere.com) attempts to appease both sides and the middle by publishing news stories three ways, with labels of \"Impartial\", \"Left\", & \"Right\" readers can choose from. This is accomplished using a combination of human editors, scraping AI, and NLP machine learning models used to encode and decode the articles according to weights. \n",
    "\n",
    "A crucial aspect of the NLP models is the summarization of published news articles. This presents a particular challenge as articles are often 700+ words. Smaller texts, like tweets, comments, and product reviews are often only a few hundred characters presenting a single idea. News articles give backstory, context, and different perspectives (even if only to marginalize that perspective). \n",
    "\n",
    "In this capstone I have attempted to create a text summarizer for 3,000 articles published in September and October 2018 on four websites, listed left to right by subjective bias in journalism from left to right Huffington Post, Politico, BBC US, and The Daily Caller. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The Daily Caller Front Page 10-24-18](dailycaller102418.png \"The Daily Caller Front Page 10-24-18\")\n",
    "\n",
    "First, I created a spider to scrape the websites across their respective front pages, going into each article to capture the headline, date, and article text. I did this every 48-72 hours using four separate .py files (dc_recursive, huffpo, politico, & bbcus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import itertools as it\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Caller Scraper using Scrapy\n",
    "\n",
    "if 1 == 0:\n",
    "    from scrapy.linkextractors import LinkExtractor\n",
    "    from scrapy import Selector\n",
    "    from dailycaller.items import DailycallerItem\n",
    "    from scrapy.spiders import Spider \n",
    "    from scrapy.crawler import CrawlerProcess \n",
    "    import scrapy\n",
    "\n",
    "    BASE_URL = 'https://dailycaller.com'\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    class QuotesSpider(scrapy.Spider): \n",
    "        name = \"dailycaller\" \n",
    "        start_urls = [BASE_URL]\n",
    "\n",
    "        def parse(self, response):\n",
    "            article_links = response.xpath('//article/a/@href')\n",
    "            for link in article_links:\n",
    "                #TODO: get a better relative link handler\n",
    "                yield scrapy.Request(BASE_URL + link.extract(), callback=self.parse_article)\n",
    "\n",
    "        def parse_article(self, response):\n",
    "            title = response.xpath('//h1/text()')[0].extract()\n",
    "            time = response.xpath('//time/text()')[0].extract()\n",
    "            text = '\\\\n\\\\n'.join(map(lambda x: x.extract(), response.xpath('//p/text()')))\n",
    "            yield {'Title': title, 'Date': time, 'Article':text}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Daily Caller csv is obtained by running a scrape in Command using Sublime text editor code:\n",
    "\n",
    "scrapy crawl dailycaller -o dcrXXXX.csv -t csv (\"XXXX\" indicating the date of the scrape) \n",
    "\n",
    "![News Scrape Using Command Prompt](scrapycommand.png \"News Scrape Using Command Prompt\")\n",
    "\n",
    "The same can be done for similar scraping code written for Politico (politico), Huffington Post (huffpo) and BBC US (bbcus). For each scrape's CSV I manually inspect the articles and eliminate those where the scrape failed to properly collect the headline or article. This was most common in BBC US articles that would often using a recurring headline for geographic areas or topics such as \"Africa\" and \"Health\", usually accounting for approximately 30% of BBC scrapes. Just as common, though to a lesser extent, about 5% of The Daily Caller articles, mostly from the \"Opinion\" section would return empty space for the article.\n",
    "\n",
    "Next I read the CSVs in and began preprocessing:\n",
    "\n",
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the DataFrames\n",
    "dcr = pd.read_csv('C:\\\\Users\\\\mkesslar\\\\Desktop\\\\Thinkful\\\\DS Bootcamp\\\\dailycaller\\\\dcr Man_Cleaned.csv')\n",
    "huffpo = pd.read_csv('C:\\\\Users\\\\mkesslar\\\\Desktop\\\\Thinkful\\\\DS Bootcamp\\\\dailycaller\\\\huffpo Man_Cleaned.csv')\n",
    "pol = pd.read_csv('C:\\\\Users\\\\mkesslar\\\\Desktop\\\\Thinkful\\\\DS Bootcamp\\\\dailycaller\\\\politico Man_Cleaned.csv')\n",
    "bbc = pd.read_csv('C:\\\\Users\\\\mkesslar\\\\Desktop\\\\Thinkful\\\\DS Bootcamp\\\\dailycaller\\\\bbcus Man_Cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1085, 3)\n",
      "(466, 3)\n",
      "(906, 3)\n",
      "(545, 3)\n"
     ]
    }
   ],
   "source": [
    "# Inspect\n",
    "print(dcr.shape)\n",
    "print(huffpo.shape)\n",
    "print(pol.shape)\n",
    "print(bbc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN’s Brian Stelter Promised Us He’d Be On Vacation This Week – What Happened?</td>\n",
       "      <td>1:03 PM 08/30/2018</td>\n",
       "      <td>CNN’s alleged media reporter \\n\\n is on vacation this week. We don’t know this because he’s suddenly missing from the political bubble. We know it because he claimed that he had temporarily disabled his Twitter feed — to, ahem, spend time with the fam — but then we were forced to see the contents of his large head spilling out into the Twittersphere.\\n\\nAll. Week. Long.\\n\\nCan’t we have seven days of no Stelter in peace?\\n\\nThe short answer is: No.\\n\\nFour days prior to the alleged vacation, \\n\\n quietly received word that Stelter would be off starting Sunday night.\\n\\nHis colleague, \\n\\n, has taken over his late-night newsletter, which he began publishing Sunday. But there’s Stelter….hovering. We are subjected to his amateur sunset photograph that insists he’s on vacation when he’s really incessantly working half the time.\\n\\nIs there anything \\n\\n annoying?\\n\\nWell, yeah.\\n\\nMaybe the worst of this Stelter windstorm arrived in our inboxes Wednesday night when he shared what amoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Confusion In Super Close Alaska Primary After 17 Voter Registrations Trace Back To A Single Mobile Home</td>\n",
       "      <td>1:10 PM 09/01/2018</td>\n",
       "      <td>A potential investigation hangs over an Alaska State House Republican primary after the race came down to a few votes, but irregularities like 17 voter registrations that trace back to a single mobile home address caught the attention of the Alaska Division of Elections.\\n\\nIncumbent Gabrielle LeDoux leads challenger Aaron Weaver \\n\\n after Tuesday’s election, but at least 26 absentee ballots for LeDoux are classified as “suspect” by the state Division of Elections, \\n\\n.\\n\\nThe state Republican Party Chairman Tuckerman Babcock is calling for an investigation. LeDoux fell out of favor with her party in 2016 after she \\n\\n.\\n\\nThe 17 Republican voter registrations linked to a single mobile home are in a section of House District 15 that is home to members of the Chinese Hmong community. A woman who answered the door of the mobile home and identified herself as Laura Chang told KTVA she did not know many of the people who registered to vote using the address. \\n\\nAnother mobile home ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     Title  \\\n",
       "0                           CNN’s Brian Stelter Promised Us He’d Be On Vacation This Week – What Happened?   \n",
       "1  Confusion In Super Close Alaska Primary After 17 Voter Registrations Trace Back To A Single Mobile Home   \n",
       "\n",
       "                 Date  \\\n",
       "0  1:03 PM 08/30/2018   \n",
       "1  1:10 PM 09/01/2018   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Article  \n",
       "0  CNN’s alleged media reporter \\n\\n is on vacation this week. We don’t know this because he’s suddenly missing from the political bubble. We know it because he claimed that he had temporarily disabled his Twitter feed — to, ahem, spend time with the fam — but then we were forced to see the contents of his large head spilling out into the Twittersphere.\\n\\nAll. Week. Long.\\n\\nCan’t we have seven days of no Stelter in peace?\\n\\nThe short answer is: No.\\n\\nFour days prior to the alleged vacation, \\n\\n quietly received word that Stelter would be off starting Sunday night.\\n\\nHis colleague, \\n\\n, has taken over his late-night newsletter, which he began publishing Sunday. But there’s Stelter….hovering. We are subjected to his amateur sunset photograph that insists he’s on vacation when he’s really incessantly working half the time.\\n\\nIs there anything \\n\\n annoying?\\n\\nWell, yeah.\\n\\nMaybe the worst of this Stelter windstorm arrived in our inboxes Wednesday night when he shared what amoun...  \n",
       "1  A potential investigation hangs over an Alaska State House Republican primary after the race came down to a few votes, but irregularities like 17 voter registrations that trace back to a single mobile home address caught the attention of the Alaska Division of Elections.\\n\\nIncumbent Gabrielle LeDoux leads challenger Aaron Weaver \\n\\n after Tuesday’s election, but at least 26 absentee ballots for LeDoux are classified as “suspect” by the state Division of Elections, \\n\\n.\\n\\nThe state Republican Party Chairman Tuckerman Babcock is calling for an investigation. LeDoux fell out of favor with her party in 2016 after she \\n\\n.\\n\\nThe 17 Republican voter registrations linked to a single mobile home are in a section of House District 15 that is home to members of the Chinese Hmong community. A woman who answered the door of the mobile home and identified herself as Laura Chang told KTVA she did not know many of the people who registered to vote using the address. \\n\\nAnother mobile home ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 1000\n",
    "dcr.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I need to strip the data of some tags and extra space. Regular Expressions affected the rest of the article in negative ways, particularly the fowardslash removal. Individually, the function works. A couple extra doublespaces are of no concern. So I used a .replace function instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A potential investigation hangs over an Alaska State House Republican primary after the race came down to a few votes, but irregularities like 17 voter registrations that trace back to a single mobile home address caught the attention of the Alaska Division of Elections.\\\\n\\\\nIncumbent Gabrielle LeDoux leads challenger Aaron Weaver \\\\n\\\\n after Tuesday’s election, but at least 26 absentee ballots for LeDoux are classified as “suspect” by the state Division of Elections, \\\\n\\\\n.\\\\n\\\\nThe state Republican Party Chairman Tuckerman Babcock is calling for an investigation. LeDoux fell out of favor with her party in 2016 after she \\\\n\\\\n.\\\\n\\\\nThe 17 Republican voter registrations linked to a single mobile home are in a section of House District 15 that is home to members of the Chinese Hmong community. A woman who answered the door of the mobile home and identified herself as Laura Chang told KTVA she did not know many of the people who registered to vote using the address.\\xa0\\\\n\\\\nAnother mobile home in the same park has 14 registered Republicans according to voter registrations, \\\\n\\\\n.\\\\n\\\\nThe irregularities in voter registrations appear to trace back to a single man, Charlie Chang. LeDoux “reportedly flew Chang up from California and paid him $10,000 to help get out the vote in the Hmong community in her district” \\\\n\\\\n, according to campaign finance reports cited by KTVA.\\\\n\\\\n“I hired him for a number of elections, for several elections, and I have no reason to think that he’s done anything wrong,” LeDoux told Anchorage Daily News.\\\\n\\\\nChang himself is registered to vote with an address in a mobile home in the Hmong neighborhood although other documentation states he lives in California, reported KTVA.\\\\n\\\\nState attorney Margaret Paton-Walsh admitted there was evidence of voter fraud Monday, \\\\n\\\\n. Elections results are set to be certified Saturday.\\\\n\\\\nLeDoux had a three-vote deficit on election night Tuesday but passed Weaver when absentee and other ballots were counted.\\\\n\\\\nCORPORATE\\\\n\\\\nCOLUMNS\\\\n\\\\nSECTIONS'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcr['Article'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify(text):\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    text = text.replace('\\\\\\\\n', ' ')\n",
    "    text = text.replace('CORPORATE', '')\n",
    "    text = text.replace('COLUMNS', '')\n",
    "    text = text.replace('SECTIONS', '')\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    text = text.replace('  ',' ')\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcr['Article_M'] = dcr['Article'].apply(modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A potential investigation hangs over an Alaska State House Republican primary after the race came down to a few votes, but irregularities like 17 voter registrations that trace back to a single mobile home address caught the attention of the Alaska Division of Elections. Incumbent Gabrielle LeDoux leads challenger Aaron Weaver  after Tuesday’s election, but at least 26 absentee ballots for LeDoux are classified as “suspect” by the state Division of Elections,  . The state Republican Party Chairman Tuckerman Babcock is calling for an investigation. LeDoux fell out of favor with her party in 2016 after she  . The 17 Republican voter registrations linked to a single mobile home are in a section of House District 15 that is home to members of the Chinese Hmong community. A woman who answered the door of the mobile home and identified herself as Laura Chang told KTVA she did not know many of the people who registered to vote using the address.  Another mobile home in the same park has 14 registered Republicans according to voter registrations,  . The irregularities in voter registrations appear to trace back to a single man, Charlie Chang. LeDoux “reportedly flew Chang up from California and paid him $10,000 to help get out the vote in the Hmong community in her district”  , according to campaign finance reports cited by KTVA. “I hired him for a number of elections, for several elections, and I have no reason to think that he’s done anything wrong,” LeDoux told Anchorage Daily News. Chang himself is registered to vote with an address in a mobile home in the Hmong neighborhood although other documentation states he lives in California, reported KTVA. State attorney Margaret Paton-Walsh admitted there was evidence of voter fraud Monday,  . Elections results are set to be certified Saturday. LeDoux had a three-vote deficit on election night Tuesday but passed Weaver when absentee and other ballots were counted.   '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcr['Article_M'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review the length of the articles\n",
    "dcr['Article_length'] = dcr['Article'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3347\n",
       "1    2018\n",
       "2    1477\n",
       "3    2054\n",
       "4    2560\n",
       "Name: Article_length, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcr['Article_length'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     1085.000000\n",
       "mean      2831.541014\n",
       "std       2087.460691\n",
       "min         62.000000\n",
       "25%       1468.000000\n",
       "50%       2162.000000\n",
       "75%       3799.000000\n",
       "max      31379.000000\n",
       "Name: Article_length, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcr['Article_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': [<matplotlib.lines.Line2D at 0x10d7a2c5b70>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x10d7a2dc5c0>,\n",
       "  <matplotlib.lines.Line2D at 0x10d7a2dc9e8>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x10d7a2e6278>],\n",
       " 'means': [],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x10d7a2dce10>],\n",
       " 'whiskers': [<matplotlib.lines.Line2D at 0x10d7a2c5cc0>,\n",
       "  <matplotlib.lines.Line2D at 0x10d7a2dc198>]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d7a16b400>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Article Lengths')\n",
    "plt.boxplot(dcr['Article_length'], 0, 'rs', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this were numerical data to be analyzed these outliers would be problematic, as would the min. However, regardless of the article lengths I am going to parse the articles so that I can clean them for vectorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcr['Articles_Parsed'] = [nlp(article) for article in dcr['Article_M']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                      (CNN, ’s, alleged, media, reporter,  , is, on, vacation, this, week, ., We, do, n’t, know, this, because, he, ’s, suddenly, missing, from, the, political, bubble, ., We, know, it, because, he, claimed, that, he, had, temporarily, disabled, his, Twitter, feed, —, to, ,, ahem, ,, spend, time, with, the, fam, —, but, then, we, were, forced, to, see, the, contents, of, his, large, head, spilling, out, into, the, Twittersphere, ., All, ., Week, ., Long, ., Ca, n’t, we, have, seven, days, of, no, Stelter, in, peace, ?, The, short, answer, is, :, No, ., Four, days, prior, to, ...)\n",
       "1    (A, potential, investigation, hangs, over, an, Alaska, State, House, Republican, primary, after, the, race, came, down, to, a, few, votes, ,, but, irregularities, like, 17, voter, registrations, that, trace, back, to, a, single, mobile, home, address, caught, the, attention, of, the, Alaska, Division, of, Elections, ., Incumbent, Gabrielle, LeDoux, leads, challenger, Aaron, Weaver,  , after, Tuesday, ’s, election, ,, but, at, least, 26, absentee, ballots, for, LeDoux, are, classified, as, “, suspect, ”, by, the, state, Division, of, Elections, ,,  , ., The, state, Republican, Party, Chairman, Tuckerman, Babcock, is, calling, for, an, investigation, ., LeDoux, fell, out, of, favor, ...)\n",
       "Name: Articles_Parsed, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcr['Articles_Parsed'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The articles parsed, I next lemmatize, lowercase, and remove stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')\n",
    "\n",
    "def cleanData(sentence):\n",
    "    processedList = \"\"\n",
    "   \n",
    "   # convert to lowercase, ignore all special characters - keep only alpha-numericals and spaces (not removing full-stop here)\n",
    "    sentence = re.sub(r'[^A-Za-z0-9\\s.]',r'',str(sentence).lower())\n",
    "    sentence = re.sub(r'\\n',r' ',sentence)\n",
    "   \n",
    "   # remove stop words\n",
    "    sentence = [word for word in sentence.split() if word not in stopWords]\n",
    "   \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords \n",
    "dcr['Articles_Cleaned'] = (dcr['Articles_Parsed']).apply(cleanData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The articles can't be fed through the Recurrant Neural Network in their current state of lists of tokens. So I merge them back together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejoin(text):\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcr['Joined'] = dcr['Articles_Cleaned'].apply(lambda x: rejoin(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cnns alleged media reporter vacation week. dont know hes suddenly missing political bubble. know claimed temporarily disabled twitter feed ahem spend time fam forced see contents large head spilling twittersphere. all. week. long. cant seven days stelter peace short answer no. four days prior alleged vacation quietly received word stelter would starting sunday night. colleague taken latenight newsletter began publishing sunday. theres stelter.hovering. subjected amateur sunset photograph insists hes vacation hes really incessantly working half time. anything annoying well yeah. maybe worst stelter windstorm arrived inboxes wednesday night shared amounts big fat lie nightly newsletter written darcy. stelter emails uninstalled twitter. fake news went town coffee newspapers. cares took sunny bay. cute still cares picked seashells. ditto fish flatbreads dinner. need know went see crazy rich asians jame loved it. really one cares gobs journalists pundits able willing bash week really need antitrump stelter stop sunny nonvacation tweet retweet peoples shit. brace pinned tweet. vacation sunset shot poking fun inability take vacation subject us irritating tweets week long tfw delete twitter vaycay brian stelter brianstelter follower femmilvet decent question multiple personalities one still tweeting. tuesday stelter took time away daughter dressed democracy dies darkness onesie infant retweet story bashes trump. wont next week clearly stelter come serious case fomo. trump cabinet. cable news cabinet hannity tucker lou dobbs course judge jeanine stelter wrote promoting piece. incidentally mentioned onesie d.c. journalist writer replied imagine stelter would stretch could wear it. stelter also felt burning need weigh trump going google ostensibly making look bad. hes running media hes running silicon valley stelter annoyingly chimed in. also tuesday stelter received message media researcher send public response. hey vacation week actually long nyc dm wrote. follower penny d. summed feelings like take permanent vacation let someone isnt partisan hack take over. great vacation brian well see return obviously ever happens. sigh.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcr['Joined'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Date', 'Article', 'Article_M', 'Article_length',\n",
       "       'Articles_Parsed', 'Articles_Cleaned', 'Joined'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unneeded columnds\n",
    "dcr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN’s Brian Stelter Promised Us He’d Be On Vacation This Week – What Happened?</td>\n",
       "      <td>1:03 PM 08/30/2018</td>\n",
       "      <td>cnns alleged media reporter vacation week. dont know hes suddenly missing political bubble. know claimed temporarily disabled twitter feed ahem spend time fam forced see contents large head spilling twittersphere. all. week. long. cant seven days stelter peace short answer no. four days prior alleged vacation quietly received word stelter would starting sunday night. colleague taken latenight newsletter began publishing sunday. theres stelter.hovering. subjected amateur sunset photograph insists hes vacation hes really incessantly working half time. anything annoying well yeah. maybe worst stelter windstorm arrived inboxes wednesday night shared amounts big fat lie nightly newsletter written darcy. stelter emails uninstalled twitter. fake news went town coffee newspapers. cares took sunny bay. cute still cares picked seashells. ditto fish flatbreads dinner. need know went see crazy rich asians jame loved it. really one cares gobs journalists pundits able willing bash week really ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Confusion In Super Close Alaska Primary After 17 Voter Registrations Trace Back To A Single Mobile Home</td>\n",
       "      <td>1:10 PM 09/01/2018</td>\n",
       "      <td>potential investigation hangs alaska state house republican primary race came votes irregularities like 17 voter registrations trace back single mobile home address caught attention alaska division elections. incumbent gabrielle ledoux leads challenger aaron weaver tuesdays election least 26 absentee ballots ledoux classified suspect state division elections . state republican party chairman tuckerman babcock calling investigation. ledoux fell favor party 2016 . 17 republican voter registrations linked single mobile home section house district 15 home members chinese hmong community. woman answered door mobile home identified laura chang told ktva know many people registered vote using address. another mobile home park 14 registered republicans according voter registrations . irregularities voter registrations appear trace back single man charlie chang. ledoux reportedly flew chang california paid 10000 help get vote hmong community district according campaign finance reports cited...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     Title  \\\n",
       "0                           CNN’s Brian Stelter Promised Us He’d Be On Vacation This Week – What Happened?   \n",
       "1  Confusion In Super Close Alaska Primary After 17 Voter Registrations Trace Back To A Single Mobile Home   \n",
       "\n",
       "                 Date  \\\n",
       "0  1:03 PM 08/30/2018   \n",
       "1  1:10 PM 09/01/2018   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Joined  \n",
       "0  cnns alleged media reporter vacation week. dont know hes suddenly missing political bubble. know claimed temporarily disabled twitter feed ahem spend time fam forced see contents large head spilling twittersphere. all. week. long. cant seven days stelter peace short answer no. four days prior alleged vacation quietly received word stelter would starting sunday night. colleague taken latenight newsletter began publishing sunday. theres stelter.hovering. subjected amateur sunset photograph insists hes vacation hes really incessantly working half time. anything annoying well yeah. maybe worst stelter windstorm arrived inboxes wednesday night shared amounts big fat lie nightly newsletter written darcy. stelter emails uninstalled twitter. fake news went town coffee newspapers. cares took sunny bay. cute still cares picked seashells. ditto fish flatbreads dinner. need know went see crazy rich asians jame loved it. really one cares gobs journalists pundits able willing bash week really ne...  \n",
       "1  potential investigation hangs alaska state house republican primary race came votes irregularities like 17 voter registrations trace back single mobile home address caught attention alaska division elections. incumbent gabrielle ledoux leads challenger aaron weaver tuesdays election least 26 absentee ballots ledoux classified suspect state division elections . state republican party chairman tuckerman babcock calling investigation. ledoux fell favor party 2016 . 17 republican voter registrations linked single mobile home section house district 15 home members chinese hmong community. woman answered door mobile home identified laura chang told ktva know many people registered vote using address. another mobile home park 14 registered republicans according voter registrations . irregularities voter registrations appear trace back single man charlie chang. ledoux reportedly flew chang california paid 10000 help get vote hmong community district according campaign finance reports cited...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['Title', 'Date', 'Joined']\n",
    "dcr = pd.DataFrame(dcr, columns=cols)\n",
    "dcr.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, I export the CSV to be used by the Python files containing the RNN, training, and predict functions. \n",
    "dcr.to_csv('C:\\\\Users\\\\mkesslar\\\\Desktop\\\\Thinkful\\\\DS Bootcamp\\\\keras-text-summarization-master\\\\demo\\\\dcr Man_Cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizations\n",
    "\n",
    "Next came a series of summarization tactics, mostly using Recurrant Neural Networks. Though all are available in the 'RNNs' folder on my git, the code below is for RNN1, the one that had the best results, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#News Loader\n",
    "if 1 == 0:\n",
    "    from collections import Counter\n",
    "\n",
    "    MAX_INPUT_SEQ_LENGTH = 500\n",
    "    MAX_TARGET_SEQ_LENGTH = 50\n",
    "    MAX_INPUT_VOCAB_SIZE = 5000\n",
    "    MAX_TARGET_VOCAB_SIZE = 2000\n",
    "\n",
    "\n",
    "    def fit_text(X, Y, input_seq_max_length=None, target_seq_max_length=None):\n",
    "        if input_seq_max_length is None:\n",
    "            input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n",
    "        if target_seq_max_length is None:\n",
    "            target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n",
    "        input_counter = Counter()\n",
    "        target_counter = Counter()\n",
    "        max_input_seq_length = 0\n",
    "        max_target_seq_length = 0\n",
    "\n",
    "        for line in X:\n",
    "            # for word in line: \n",
    "            #     if isinstance(word, float): print(\"ERROR IS HERE\", word)\n",
    "            text = [word.lower() for word in line.split(' ')]\n",
    "            seq_length = len(text)\n",
    "            if seq_length > input_seq_max_length:\n",
    "                text = text[0:input_seq_max_length]\n",
    "                seq_length = len(text)\n",
    "            for word in text:\n",
    "                input_counter[word] += 1\n",
    "            max_input_seq_length = max(max_input_seq_length, seq_length)\n",
    "\n",
    "        for line in Y:\n",
    "            line2 = 'START ' + line.lower() + ' END'\n",
    "            text = [word for word in line2.split(' ')]\n",
    "            seq_length = len(text)\n",
    "            if seq_length > target_seq_max_length:\n",
    "                text = text[0:target_seq_max_length]\n",
    "                seq_length = len(text)\n",
    "            for word in text:\n",
    "                target_counter[word] += 1\n",
    "                max_target_seq_length = max(max_target_seq_length, seq_length)\n",
    "\n",
    "        input_word2idx = dict()\n",
    "        for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n",
    "            input_word2idx[word[0]] = idx + 2\n",
    "        input_word2idx['PAD'] = 0\n",
    "        input_word2idx['UNK'] = 1\n",
    "        input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
    "\n",
    "        target_word2idx = dict()\n",
    "        for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n",
    "            target_word2idx[word[0]] = idx + 1\n",
    "        target_word2idx['UNK'] = 0\n",
    "\n",
    "        target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "\n",
    "        num_input_tokens = len(input_word2idx)\n",
    "        num_target_tokens = len(target_word2idx)\n",
    "\n",
    "        config = dict()\n",
    "        config['input_word2idx'] = input_word2idx\n",
    "        config['input_idx2word'] = input_idx2word\n",
    "        config['target_word2idx'] = target_word2idx\n",
    "        config['target_idx2word'] = target_idx2word\n",
    "        config['num_input_tokens'] = num_input_tokens\n",
    "        config['num_target_tokens'] = num_target_tokens\n",
    "        config['max_input_seq_length'] = max_input_seq_length\n",
    "        config['max_target_seq_length'] = max_target_seq_length\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN1\n",
    "\n",
    "if 1 == 0:\n",
    "    class RecursiveRNN1(object):\n",
    "        model_name = 'recursive-rnn-1'\n",
    "        \"\"\"\n",
    "        A second alternative model is to develop a model that generates a single word forecast and call it recursively.\n",
    "\n",
    "        That is, the decoder uses the context vector and the distributed representation of all words generated so far as \n",
    "        input in order to generate the next word. \n",
    "\n",
    "        A language model can be used to interpret the sequence of words generated so far to provide a second context vector \n",
    "        to combine with the representation of the source document in order to generate the next word in the sequence.\n",
    "\n",
    "        The summary is built up by recursively calling the model with the previously generated word appended (or, more \n",
    "        specifically, the expected previous word during training).\n",
    "\n",
    "        The context vectors could be concentrated or added together to provide a broader context for the decoder to \n",
    "        interpret and output the next word.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, config):\n",
    "            self.num_input_tokens = config['num_input_tokens']\n",
    "            self.max_input_seq_length = config['max_input_seq_length']\n",
    "            self.num_target_tokens = config['num_target_tokens']\n",
    "            self.max_target_seq_length = config['max_target_seq_length']\n",
    "            self.input_word2idx = config['input_word2idx']\n",
    "            self.input_idx2word = config['input_idx2word']\n",
    "            self.target_word2idx = config['target_word2idx']\n",
    "            self.target_idx2word = config['target_idx2word']\n",
    "            if 'version' in config:\n",
    "                self.version = config['version']\n",
    "            else:\n",
    "                self.version = 0\n",
    "            self.config = config\n",
    "\n",
    "            print('max_input_seq_length', self.max_input_seq_length)\n",
    "            print('max_target_seq_length', self.max_target_seq_length)\n",
    "            print('num_input_tokens', self.num_input_tokens)\n",
    "            print('num_target_tokens', self.num_target_tokens)\n",
    "\n",
    "            inputs1 = Input(shape=(self.max_input_seq_length,))\n",
    "            am1 = Embedding(self.num_input_tokens, 128)(inputs1)\n",
    "            #am2 = LSTM(128)(am1)\n",
    "            am2 = LSTM(128, kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(am1) #regs added\n",
    "\n",
    "            inputs2 = Input(shape=(self.max_target_seq_length,))\n",
    "            sm1 = Embedding(self.num_target_tokens, 128)(inputs2)\n",
    "            #sm2 = LSTM(128)(sm1)\n",
    "            sm2 = LSTM(128, kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(sm1) #regs added\n",
    "\n",
    "            decoder1 = concatenate([am2, sm2])\n",
    "            outputs = Dense(self.num_target_tokens, activation='softmax')(decoder1)\n",
    "\n",
    "            model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\n",
    "            model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            self.model = model\n",
    "\n",
    "        def load_weights(self, weight_file_path):\n",
    "            if os.path.exists(weight_file_path):\n",
    "                self.model.load_weights(weight_file_path)\n",
    "\n",
    "        def transform_input_text(self, texts):\n",
    "            temp = []\n",
    "            for line in texts:\n",
    "                x = []\n",
    "                for word in line.lower().split(' '):\n",
    "                    wid = 1\n",
    "                    if word in self.input_word2idx:\n",
    "                        wid = self.input_word2idx[word]\n",
    "                    x.append(wid)\n",
    "                    if len(x) >= self.max_input_seq_length:\n",
    "                        break\n",
    "                temp.append(x)\n",
    "            temp = pad_sequences(temp, maxlen=self.max_input_seq_length)\n",
    "\n",
    "            print(temp.shape)\n",
    "            return temp\n",
    "\n",
    "        def split_target_text(self, texts):\n",
    "            temp = []\n",
    "            for line in texts:\n",
    "                x = []\n",
    "                line2 = 'START ' + line.lower() + ' END'\n",
    "                for word in line2.split(' '):\n",
    "                    x.append(word)\n",
    "                    if len(x)+1 >= self.max_target_seq_length:\n",
    "                        x.append('END')\n",
    "                        break\n",
    "                temp.append(x)\n",
    "            return temp\n",
    "\n",
    "        def generate_batch(self, x_samples, y_samples, batch_size):\n",
    "            encoder_input_data_batch = []\n",
    "            decoder_input_data_batch = []\n",
    "            decoder_target_data_batch = []\n",
    "            line_idx = 0\n",
    "            while True:\n",
    "                for recordIdx in range(0, len(x_samples)):\n",
    "                    target_words = y_samples[recordIdx]\n",
    "                    x = x_samples[recordIdx]\n",
    "                    decoder_input_line = []\n",
    "\n",
    "                    for idx in range(0, len(target_words)-1):\n",
    "                        w2idx = 0  # default [UNK]\n",
    "                        w = target_words[idx]\n",
    "                        if w in self.target_word2idx:\n",
    "                            w2idx = self.target_word2idx[w]\n",
    "                        decoder_input_line = decoder_input_line + [w2idx]\n",
    "                        decoder_target_label = np.zeros(self.num_target_tokens)\n",
    "                        w2idx_next = 0\n",
    "                        if target_words[idx+1] in self.target_word2idx:\n",
    "                            w2idx_next = self.target_word2idx[target_words[idx+1]]\n",
    "                        if w2idx_next != 0:\n",
    "                            decoder_target_label[w2idx_next] = 1\n",
    "                        decoder_input_data_batch.append(decoder_input_line)\n",
    "                        encoder_input_data_batch.append(x)\n",
    "                        decoder_target_data_batch.append(decoder_target_label)\n",
    "\n",
    "                        line_idx += 1\n",
    "                        if line_idx >= batch_size:\n",
    "                            yield [pad_sequences(encoder_input_data_batch, self.max_input_seq_length),\n",
    "                                   pad_sequences(decoder_input_data_batch,\n",
    "                                                 self.max_target_seq_length)], np.array(decoder_target_data_batch)\n",
    "                            line_idx = 0\n",
    "                            encoder_input_data_batch = []\n",
    "                            decoder_input_data_batch = []\n",
    "                            decoder_target_data_batch = []\n",
    "\n",
    "        @staticmethod\n",
    "        def get_weight_file_path(model_dir_path):\n",
    "            return model_dir_path + '/' + RecursiveRNN1.model_name + '-weights.h5'\n",
    "\n",
    "        @staticmethod\n",
    "        def get_config_file_path(model_dir_path):\n",
    "            return model_dir_path + '/' + RecursiveRNN1.model_name + '-config.npy'\n",
    "\n",
    "        @staticmethod\n",
    "        def get_architecture_file_path(model_dir_path):\n",
    "            return model_dir_path + '/' + RecursiveRNN1.model_name + '-architecture.json'\n",
    "\n",
    "        def fit(self, Xtrain, Ytrain, Xtest, Ytest, epochs=None, model_dir_path=None, batch_size=None):\n",
    "            if epochs is None:\n",
    "                epochs = DEFAULT_EPOCHS\n",
    "            if model_dir_path is None:\n",
    "                model_dir_path = './models'\n",
    "            if batch_size is None:\n",
    "                batch_size = DEFAULT_BATCH_SIZE\n",
    "\n",
    "            self.version += 1\n",
    "            self.config['version'] = self.version\n",
    "\n",
    "            config_file_path = RecursiveRNN1.get_config_file_path(model_dir_path)\n",
    "            weight_file_path = RecursiveRNN1.get_weight_file_path(model_dir_path)\n",
    "            checkpoint = ModelCheckpoint(weight_file_path)\n",
    "            np.save(config_file_path, self.config)\n",
    "            architecture_file_path = RecursiveRNN1.get_architecture_file_path(model_dir_path)\n",
    "            open(architecture_file_path, 'w').write(self.model.to_json())\n",
    "\n",
    "            Ytrain = self.split_target_text(Ytrain)\n",
    "            Ytest = self.split_target_text(Ytest)\n",
    "\n",
    "            Xtrain = self.transform_input_text(Xtrain)\n",
    "            Xtest = self.transform_input_text(Xtest)\n",
    "\n",
    "            train_gen = self.generate_batch(Xtrain, Ytrain, batch_size)\n",
    "            test_gen = self.generate_batch(Xtest, Ytest, batch_size)\n",
    "\n",
    "            total_training_samples = sum([len(target_text)-1 for target_text in Ytrain])\n",
    "            total_testing_samples = sum([len(target_text)-1 for target_text in Ytest])\n",
    "            train_num_batches = total_training_samples // batch_size\n",
    "            test_num_batches = total_testing_samples // batch_size\n",
    "\n",
    "            history = self.model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                                               epochs=epochs,\n",
    "                                               verbose=VERBOSE, validation_data=test_gen, validation_steps=test_num_batches,\n",
    "                                               callbacks=[checkpoint])\n",
    "            self.model.save_weights(weight_file_path)\n",
    "            return history\n",
    "\n",
    "        def summarize(self, input_text):\n",
    "            input_seq = []\n",
    "            input_wids = []\n",
    "            for word in input_text.lower().split(' '):\n",
    "                idx = 1  # default [UNK]\n",
    "                if word in self.input_word2idx:\n",
    "                    idx = self.input_word2idx[word]\n",
    "                input_wids.append(idx)\n",
    "            input_seq.append(input_wids)\n",
    "            input_seq = pad_sequences(input_seq, self.max_input_seq_length)\n",
    "            start_token = self.target_word2idx['START']\n",
    "            wid_list = [start_token]\n",
    "            sum_input_seq = pad_sequences([wid_list], self.max_target_seq_length)\n",
    "            terminated = False\n",
    "\n",
    "            target_text = ''\n",
    "\n",
    "            while not terminated:\n",
    "                output_tokens = self.model.predict([input_seq, sum_input_seq])\n",
    "                sample_token_idx = np.argmax(output_tokens[0, :])\n",
    "                sample_word = self.target_idx2word[sample_token_idx]\n",
    "                wid_list = wid_list + [sample_token_idx]\n",
    "\n",
    "                if sample_word != 'START' and sample_word != 'END':\n",
    "                    target_text += ' ' + sample_word\n",
    "\n",
    "                if sample_word == 'END' or len(wid_list) >= self.max_target_seq_length:\n",
    "                    terminated = True\n",
    "                else:\n",
    "                    sum_input_seq = pad_sequences([wid_list], self.max_target_seq_length)\n",
    "            return target_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on the Article Dataframe\n",
    "if 1 == 0:\n",
    "    #from __future__ import print_function\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from keras_text_summarization.library.utility.plot_utils import plot_and_save_history\n",
    "    from keras_text_summarization.library.rnn import RecursiveRNN1\n",
    "    from keras_text_summarization.library.applications.news_loader import fit_text\n",
    "    import numpy as np\n",
    "\n",
    "    LOAD_EXISTING_WEIGHTS = False\n",
    "\n",
    "\n",
    "    def main():\n",
    "        np.random.seed(42)\n",
    "        data_dir_path = './data'\n",
    "        report_dir_path = './reports'\n",
    "        model_dir_path = './models'\n",
    "\n",
    "        print('loading csv file ...')\n",
    "        df = pd.read_csv(\"dcr Man_Cleaned.csv\")\n",
    "\n",
    "        print('extract configuration from input texts ...')\n",
    "        Y = df.Title\n",
    "        X = df['Joined']\n",
    "        config = fit_text(X, Y)\n",
    "\n",
    "        print('configuration extracted from input texts ...')\n",
    "\n",
    "        summarizer = RecursiveRNN1(config)\n",
    "\n",
    "        if LOAD_EXISTING_WEIGHTS:\n",
    "            weight_file_path = RecursiveRNN1.get_weight_file_path(model_dir_path=model_dir_path)\n",
    "            summarizer.load_weights(weight_file_path=weight_file_path)\n",
    "\n",
    "        Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "\n",
    "        print('demo size: ', len(Xtrain))\n",
    "        print('testing size: ', len(Xtest))\n",
    "\n",
    "        print('start fitting ...')\n",
    "        history = summarizer.fit(Xtrain, Ytrain, Xtest, Ytest, epochs=20)\n",
    "\n",
    "        history_plot_file_path = report_dir_path + '/' + RecursiveRNN1.model_name + '-history.png'\n",
    "        if LOAD_EXISTING_WEIGHTS:\n",
    "            history_plot_file_path = report_dir_path + '/' + RecursiveRNN1.model_name + '-history-v' + str(summarizer.version) + '.png'\n",
    "        plot_and_save_history(history, summarizer.model_name, history_plot_file_path, metrics={'loss', 'acc'})\n",
    "\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizations in the form of predictions against the article headlines\n",
    "if 1 == 0:\n",
    "    import pandas as pd\n",
    "    from keras_text_summarization.library.rnn import RecursiveRNN1\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "    def main():\n",
    "        np.random.seed(42)\n",
    "        data_dir_path = './data'\n",
    "        model_dir_path = './models'\n",
    "\n",
    "        print('loading csv file ...')\n",
    "        df = pd.read_csv(\"dcr Man_Cleaned.csv\")\n",
    "        # df = df.loc[df.index < 1000]\n",
    "        X = df['Joined']\n",
    "        Y = df.Title\n",
    "\n",
    "        config = np.load(RecursiveRNN1.get_config_file_path(model_dir_path=model_dir_path)).item()\n",
    "\n",
    "        summarizer = RecursiveRNN1(config)\n",
    "        summarizer.load_weights(weight_file_path=RecursiveRNN1.get_weight_file_path(model_dir_path=model_dir_path))\n",
    "\n",
    "        print('start predicting ...')\n",
    "        for i in np.random.permutation(np.arange(len(X)))[0:20]:\n",
    "            x = X[i]\n",
    "            actual_headline = Y[i]\n",
    "            headline = summarizer.summarize(x)\n",
    "            # print('Article: ', x)\n",
    "            print('Generated Headline: ', headline)\n",
    "            print('Original Headline: ', actual_headline)\n",
    "\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below are the generated headlines versus the published headlines for numerous permutations using a variety of RNN layer configurations, regularization, and optimization.\n",
    "\n",
    "## Each of these took between 6-18 hours on my laptop to run. After a while I learned to see the results headed in the wrong direction using the accuracy, loss, validation accuracy, and validation loss.\n",
    "\n",
    "## Though predictions were done on more than three articles, I have excluded all but the first three predictions for space. \n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained using only \"keras_text_summarization.library.seq2seq import Seq2SeqSummarizer\" and predicted against dcr['Joined'] (80/20 split)\n",
    "\n",
    "<br><br>Generated Headline:  opinion: band of to species of of but but of\n",
    "<br>Original Headline:  Over 2.5 Times More People Were Killed In Latin America And The Caribbean Than In Syria, Iraq And Afghanistan Combined\n",
    "<br><br>Generated Headline:  kavanaugh calls calls kavanaugh kavanaugh kavanaugh kavanaugh after kavanaugh for for for the\n",
    "<br>Original Headline:  USA Today Deletes Tweet, Edits Column After Columnist Writes Kavanaugh ‘Should Stay Off Basketball Courts’ With Kids\n",
    "<br><br>Generated Headline:  trump says says for for for kavanaugh for\n",
    "<br>Original Headline:  Trump Condemns Democrats For Being ‘Unfair’ To Kavanaugh’s Family\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using basic summarizer from [Machine Learning Master](https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras) I got the following results:\n",
    "\n",
    "<br><br>Generated Headline:  is you really\n",
    "<br>Original Headline:  Sen. Manchin Says ‘Not A Penny’ Of Planned Parenthood Funding Goes To Abortions\n",
    "<br><br>Generated Headline:  why the top us was caught to expect trump\n",
    "<br>Original Headline:  Chris Cuomo To Florida’s Andrew Gillum On Health-Care Plan: ‘You Don’t Know Exactly How To Pay For It Yet’\n",
    "<br><br>Generated Headline:  the end of america\n",
    "<br>Original Headline:  Journalists Trash Porn Star Attorney For His Dubious Kavanaugh Allegations\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "### Obviously these are not adequate. Move on to the GloVe embeddings model, then the v2 of that if that isn't adequate. \n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained with GloVe word embeddings and predicted against dcr['Joined'] (80/20 split)\n",
    "\n",
    "<br>Generated Headline:  opinion: the the the the to to to\n",
    "<br>Original Headline:  Ford Traveled To Maryland In August Despite Allegedly Fear Of Flying\n",
    "<br><br>Generated Headline:  opinion: the the the the to to\n",
    "<br>Original Headline:  OPINION: Feinstein Is The True Villain Of The Kavanaugh Hearings\n",
    "<br><br>Generated Headline:  opinion: the the the the to to to\n",
    "<br>Original Headline:  Can You Guess The Party Of These Obscure Lawmakers Just By Looking At Them?\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of dcr summaryization with GloVe V2:\n",
    "\n",
    "<br>Generated Headline:  the the not worry (opinion) to dems there transfer office demands demands picture continues continues continues continues continues continues continues continues continues continues blasts\n",
    "<br>Original Headline:  Michael Cohen’s GoFundMe Page Attracts Pranksters<br>\n",
    "<br><br>Generated Headline:  the the the is\n",
    "<br>Original Headline:  OPINION: Does The Turkish Intelligence Agency Plan To Abduct Turkish Dissidents From The US?\n",
    "<br><br>Generated Headline:  the the not worry (opinion) to dems there transfer office demands demands picture continues continues continues continues continues continues continues continues continues continues blasts\n",
    "<br>Original Headline:  Avenatti Explains Why His Client Went Back To Gang Rape Parties\n",
    "\n",
    "---\n",
    "\n",
    "### So... Not exactly concise and clear. \n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RNN 1 for the first time:\n",
    "\n",
    "<br>Generated Headline:  trump says trump for his thoughts on the ‘economic miracle’\n",
    "<br>Original Headline:  MSNBC’s Chris Matthews Says Democrats Won’t Listen To Him Anymore Because He’s Too Old\n",
    "<br><br>Generated Headline:  opinion: the fbi on the fbi on kavanaugh should the graham\n",
    "<br>Original Headline:  OPINION: The Nation And The Senate Need More Lindsey Graham Moments\n",
    "<br><br>Generated Headline:  driver intentionally rams truck into a mental illness\n",
    "<br>Original Headline:  Source: FBI Agent Told Congress The Bureau Used Leaked Stories To Obtain Spy Warrants\n",
    "\n",
    "\n",
    "### This got the following scoring at the end, after four hours of processingn 20 epochs:\n",
    "\n",
    "Epoch 20/20\n",
    "143/143 - 746s 5s/step - loss: 1.8832 - acc: 0.4327 - val_loss: 6.2781 - val_acc: 0.1357\n",
    "\n",
    "### Maybe it just needs more epochs to train on. Try the next one. \n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 2 with layer descriptions\n",
    "\n",
    "|Layer (type)                    |Output Shape         |Param #     |Connected to|\n",
    "| --- | --- | --- | --- |\n",
    "|input_2 (InputLayer)            |(None, 4)            |0 |\n",
    "|embedding_2 (Embedding)         |(None, 4, 128)       |256128      |input_2\n",
    "|input_1 (InputLayer)            |(None, 500)          |0 |\n",
    "|dropout_2 (Dropout)             |(None, 4, 128)       |           ||\n",
    "|embedding_1 (Embedding)         |(None, 500, 128)     |640256      |input_1|\n",
    "|lstm_1 (LSTM)                   |(None, 128)          |131584      |dropout_2|\n",
    "|dropout_1 (Dropout)             |(None, 500, 128)     |0           |embedding_1|\n",
    "|repeat_vector_1 (RepeatVector)  |(None, 500, 128)     |0           |lstm_1|\n",
    "|concatenate_1 (Concatenate)     |(None, 500, 256)     |0           |dropout_1|\n",
    "|                                |                     |            |repeat_vector_1|\n",
    "|lstm_2 (LSTM)                   |(None, 128)          |197120      |concatenate_1|\n",
    "|dense_1 (Dense)                 |None, 2001)         |258129      |lstm_2|\n",
    "\n",
    "<br>Total params: 1,483,217|Layer (type)                    |Output Shape         |Param #     |Connected to|\n",
    "<br>Trainable params: 1,483,217\n",
    "<br>Non-trainable params: 0\n",
    "__________________________________________________________________________________________________\n",
    "<br>None\n",
    "<br>demo size:  754\n",
    "<br> size:  189\n",
    "<br>start fitting ...\n",
    "<br>(754, 500)\n",
    "<br>189, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN2 Output\n",
    "\n",
    "<br>Generated Headline:  opinion: the the the the his his his his own\n",
    "<br>Original Headline:  OPINION: Mikheil Saakashvili: Corruption Knows No Party\n",
    "<br><br>Generated Headline:  opinion: the the the the his his his his own\n",
    "<br>Original Headline:  CNN’s April Ryan Accuses Trump White House Of Trying To ‘Kill’ Her Career Because She’s A Black Woman\n",
    "<br><br>Generated Headline:  opinion: the the the the his his his own\n",
    "<br>Original Headline:  OPINION: Make Patents Great Again\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 3\n",
    "\n",
    "|Layer (type)                    |Output Shape         |Param #     |Connected to|\n",
    "| --- | --- | --- | --- |\n",
    "|input_1 (InputLayer)            |(None, 500)            |0 |\n",
    "|input_2 (InputLayer)            |(None, 24)           |0 |\n",
    "|embedding_1 (Embedding)         |(None, 500, 128)     |640256      |input_1|\n",
    "|embedding_2 (Embedding)         |(None, 24, 128)      |256128      |input_2|\n",
    "|lstm_1 (LSTM)                   |(None, 128)          |131584      |embedding_1|\n",
    "|lstm_2 (LSTM)                   |(None, 128)          |131584      |embedding_2|\n",
    "|repeat_vector_1 (RepeatVector)  |(None, 128, 128)     |0           |lstm_1|\n",
    "|repeat_vector_2 (RepeatVector)  |(None, 128, 128)     |0           |lstm_2|\n",
    "|concatenate_1 (Concatenate)     |(None, 128, 256)     |0           |repeat_vector_1|\n",
    "|                                |                     |            |repeat_vector_2|\n",
    "|lstm_3 (LSTM)                   |(None, 128)          |197120      |concatenate_1|\n",
    "|dense_1 (Dense)                 |None, 2001)         |258129      |lstm_2|\n",
    "\n",
    "\n",
    "<br>params: 1,614,801\n",
    "<br>Trainable params: 1,614,801\n",
    "<br>-trainable params: 0\n",
    "<br>None\n",
    "<br>demo size:  754\n",
    "<br>testing size:  189\n",
    "<br>start fitting ...\n",
    "<br>754, 500)\n",
    "<br>(189, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN3 Output\n",
    "\n",
    "<br>Generated Headline:\n",
    "<br>Original Headline:  Would Gutierrez Get Arrested To Stop Kavanaugh? Maybe. But He’s Definitely Not Doing It On His Own\n",
    "<br><br>Generated Headline:\n",
    "<br>Original Headline:  Kavanaugh Protesters Refuse To Denounce Alleged Domestic Abuser Rep. Keith Ellison [VIDEO]\n",
    "<br><br>Generated Headline:\n",
    "<br>Original Headline:  Archbishop Who Accused The Pope Refutes Allegations That He Also Covered Up Sexual Misconduct\n",
    "\n",
    "\n",
    "### The model literally generated no headlines. TIme to go back to the only one that did and see if I can increase the acuracy with more epochs\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN v.1 with 40 epochs. \n",
    "\n",
    "<br><br>Generated Headline:  hollywood will stomach joe and mika over steve bannon\n",
    "<br>Original Headline:  MSNBC’s Chris Matthews Says Democrats Won’t Listen To Him Anymore Because He’s Too Old\n",
    "<br><br>Generated Headline:  opinion: the nation and the senate need more lindsey graham\n",
    "<br>Original Headline:  OPINION: The Nation And The Senate Need More Lindsey Graham Moments\n",
    "<br><br>Generated Headline:  source: fbi agent told congress to bureau used leaked stories to obtain spy warrants\n",
    "<br>Original Headline:  Source: FBI Agent Told Congress The Bureau Used Leaked Stories To Obtain Spy Warrants\n",
    "\n",
    "\n",
    "\n",
    "### This is clearly the best one yet. But still a long way from accurate. Here are the accuracy and loss scores for the 40 epochs:\n",
    "\n",
    "<br>Epoch 1/40\n",
    "143/143  - 599s 4s/step - loss: 5.6177 - acc: 0.0807 - val_loss: 5.4653 - val_acc: 0.0839\n",
    "<br>Epoch 2/40\n",
    "143/143  - 625s 4s/step - loss: 5.1302 - acc: 0.0915 - val_loss: 5.4112 - val_acc: 0.1094\n",
    "<br>Epoch 3/40\n",
    "143/143  - 648s 5s/step - loss: 4.8848 - acc: 0.1053 - val_loss: 5.5161 - val_acc: 0.1085\n",
    "<br>Epoch 4/40\n",
    "143/143  - 676s 5s/step - loss: 4.6458 - acc: 0.1193 - val_loss: 5.5397 - val_acc: 0.1080\n",
    "<br>Epoch 5/40\n",
    "143/143  - 1372s 10s/step - loss: 4.4186 - acc: 0.1260 - val_loss: 5.5322 - val_acc: 0.1071\n",
    "<br>Epoch 6/40\n",
    "143/143  - 697s 5s/step - loss: 4.2379 - acc: 0.1360 - val_loss: 5.5393 - val_acc: 0.1183\n",
    "<br>Epoch 7/40\n",
    "143/143  - 705s 5s/step - loss: 4.0412 - acc: 0.1460 - val_loss: 5.6125 - val_acc: 0.1263\n",
    "<br>Epoch 8/40\n",
    "143/143  - 688s 5s/step - loss: 3.8996 - acc: 0.1555 - val_loss: 5.6624 - val_acc: 0.1232\n",
    "<br>Epoch 9/40\n",
    "143/143  - 694s 5s/step - loss: 3.6972 - acc: 0.1672 - val_loss: 5.7194 - val_acc: 0.1183\n",
    "<br>Epoch 10/40\n",
    "143/143  - 706s 5s/step - loss: 3.4720 - acc: 0.1824 - val_loss: 5.8100 - val_acc: 0.1228\n",
    "<br>Epoch 11/40\n",
    "143/143  - 705s 5s/step - loss: 3.2418 - acc: 0.2071 - val_loss: 5.8751 - val_acc: 0.1228\n",
    "<br>Epoch 12/40\n",
    "143/143  - 690s 5s/step - loss: 3.0259 - acc: 0.2368 - val_loss: 5.9196 - val_acc: 0.1223\n",
    "<br>Epoch 13/40\n",
    "143/143  - 692s 5s/step - loss: 2.8375 - acc: 0.2693 - val_loss: 5.9408 - val_acc: 0.1268\n",
    "<br>Epoch 14/40\n",
    "143/143  - 703s 5s/step - loss: 2.6389 - acc: 0.2999 - val_loss: 6.0710 - val_acc: 0.1219\n",
    "<br>Epoch 15/40\n",
    "143/143  - 707s 5s/step - loss: 2.4306 - acc: 0.3311 - val_loss: 6.1100 - val_acc: 0.1246\n",
    "<br>Epoch 16/40\n",
    "143/143  - 699s 5s/step - loss: 2.2502 - acc: 0.3724 - val_loss: 6.1433 - val_acc: 0.1379\n",
    "<br>Epoch 17/40\n",
    "143/143  - 708s 5s/step - loss: 2.0894 - acc: 0.4018 - val_loss: 6.1841 - val_acc: 0.1388\n",
    "<br>Epoch 18/40\n",
    "143/143  - 708s 5s/step - loss: 1.9360 - acc: 0.4329 - val_loss: 6.2407 - val_acc: 0.1464\n",
    "<br>Epoch 19/40\n",
    "143/143  - 717s 5s/step - loss: 1.7629 - acc: 0.4631 - val_loss: 6.2727 - val_acc: 0.1451\n",
    "<br>Epoch 20/40\n",
    "143/143  - 713s 5s/step - loss: 1.6241 - acc: 0.4962 - val_loss: 6.3422 - val_acc: 0.1442\n",
    "<br>Epoch 21/40\n",
    "143/143  - 704s 5s/step - loss: 1.4877 - acc: 0.5282 - val_loss: 6.3821 - val_acc: 0.1402\n",
    "<br>Epoch 22/40\n",
    "143/143  - 694s 5s/step - loss: 1.3772 - acc: 0.5490 - val_loss: 6.4199 - val_acc: 0.1415\n",
    "<br>Epoch 23/40\n",
    "143/143  - 712s 5s/step - loss: 1.2572 - acc: 0.5766 - val_loss: 6.4737 - val_acc: 0.1491\n",
    "<br>Epoch 24/40\n",
    "143/143  - 694s 5s/step - loss: 1.1788 - acc: 0.5912 - val_loss: 6.5002 - val_acc: 0.1518\n",
    "<br>Epoch 25/40\n",
    "143/143  - 701s 5s/step - loss: 1.1558 - acc: 0.5972 - val_loss: 6.5219 - val_acc: 0.1540\n",
    "<br>Epoch 26/40\n",
    "143/143  - 694s 5s/step - loss: 1.0372 - acc: 0.6241 - val_loss: 6.6068 - val_acc: 0.1545\n",
    "<br>Epoch 27/40\n",
    "143/143  - 693s 5s/step - loss: 0.9361 - acc: 0.6503 - val_loss: 6.6485 - val_acc: 0.1509\n",
    "<br>Epoch 28/40\n",
    "143/143  - 696s 5s/step - loss: 0.8440 - acc: 0.6750 - val_loss: 6.6989 - val_acc: 0.1500\n",
    "<br>Epoch 29/40\n",
    "143/143  - 701s 5s/step - loss: 0.7650 - acc: 0.6945 - val_loss: 6.7226 - val_acc: 0.1545\n",
    "<br>Epoch 30/40\n",
    "143/143  - 695s 5s/step - loss: 0.6957 - acc: 0.7118 - val_loss: 6.8028 - val_acc: 0.1594\n",
    "<br>Epoch 31/40\n",
    "143/143  - 695s 5s/step - loss: 0.6323 - acc: 0.7280 - val_loss: 6.8239 - val_acc: 0.1580\n",
    "<br>Epoch 32/40\n",
    "143/143  - 703s 5s/step - loss: 0.5725 - acc: 0.7379 - val_loss: 6.8497 - val_acc: 0.1513\n",
    "<br>Epoch 33/40\n",
    "143/143  - 706s 5s/step - loss: 0.5118 - acc: 0.7525 - val_loss: 6.9105 - val_acc: 0.1429\n",
    "<br>Epoch 34/40\n",
    "143/143  - 699s 5s/step - loss: 0.4516 - acc: 0.7670 - val_loss: 6.9284 - val_acc: 0.1549\n",
    "<br>Epoch 35/40\n",
    "143/143  - 698s 5s/step - loss: 0.4045 - acc: 0.7784 - val_loss: 6.9797 - val_acc: 0.1616\n",
    "<br>Epoch 36/40\n",
    "143/143  - 704s 5s/step - loss: 0.3686 - acc: 0.7868 - val_loss: 7.0106 - val_acc: 0.1607\n",
    "<br>Epoch 37/40\n",
    "143/143  - 705s 5s/step - loss: 0.3263 - acc: 0.7969 - val_loss: 7.0893 - val_acc: 0.1473\n",
    "<br>Epoch 38/40\n",
    "143/143  - 695s 5s/step - loss: 0.2918 - acc: 0.8033 - val_loss: 7.1221 - val_acc: 0.1491\n",
    "<br>Epoch 39/40\n",
    "143/143  - 705s 5s/step - loss: 0.2557 - acc: 0.8122 - val_loss: 7.1429 - val_acc: 0.1545\n",
    "<br>Epoch 40/40\n",
    "143/143  - 705s 5s/step - loss: 0.2227 - acc: 0.8176 - val_loss: 7.2017 - val_acc: 0.1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a few thoughts here. First, the val_acc is staying very low and the val_loss is going up. But that may not necessarily mean overfitting, though that is a safe first assumption. Also, the training accuracy continues a very steady rise of 1.5-3% per epoch. This tapered off in the last ten epochs though. So if I were to train again on 60 epochs it very well might have gotten to the 90s. \n",
    "\n",
    "Next, I'm concerned about how many of the generated headlines are so identical to the original. For example: \": Afternoon Mirror: Eric Bolling Gets Wretched Note About His Son (And Responds)\". With the exception of capitalization, this is identical. And that is worrisome. I am training to the headline. The data so overfitted as to make the generated headline identical to the original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of RNN v1 with 40 epochs on cleaned data, no regularization\n",
    "\n",
    "<br><br>Generated Headline:  journalists trash journalists for kavanaugh allegations made cryptic dubious kavanaugh allegations\n",
    "<br>Original Headline:  Journalists Trash Porn Star Attorney For His Dubious Kavanaugh Allegations\n",
    "<br><br>Generated Headline:  cnn panel condemns cnn between scalise shooting and leftist protesters\n",
    "<br>Original Headline:  CNN Panel Condemns Comparison Between Scalise Shooting And Leftist Protesters\n",
    "<br><br>Generated Headline:  over the nation and and were killed in the mob in the people in than killed in afghanistan and afghanistan and the america\n",
    "<br>Original Headline:  Over 2.5 Times More People Were Killed In Latin America And The Caribbean Than In Syria, Iraq And Afghanistan Combined\n",
    "\n",
    "\n",
    "### Again, clearly overfitted. The validation loss goes up and accuracy stays very low whyle training loss goes down and accuracy climbs from 8% to 85%. Now add a regularization to the LSTMs and try with just 20 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "### At the start of the RNN things are different right away:\n",
    "\n",
    "<br>7/177 [>.............................] - ETA: 13:24 - loss: 10.8475 - acc: 0.0045\n",
    "\n",
    "The accuracy is nonexistent and the loss is high. Interested to see how this changes.\n",
    "\n",
    "### End of epoch 1...\n",
    "\n",
    "<br>loss: 7.3660 - acc: 0.0757 - val_loss: 6.3552 - val_acc: 0.0831\n",
    "\n",
    "### vs end of epoch 1 on previous, no regularization, epoch...\n",
    "\n",
    "<br>loss: 5.6177 - acc: 0.0807 - val_loss: 5.4653 - val_acc: 0.0839 \n",
    "\n",
    "### So far, no real impact that I can see. Is the regularization active? Does it matter? Only further epochs will tell. \n",
    "\n",
    "### I've stopped the training after 11 epochs as there is no increase in accuracy:\n",
    "\n",
    "<br>Epoch 2/20\n",
    "<br>177/177 [==============================] - 905s 5s/step - loss: 6.2403 - acc: 0.0820 - val_loss: 6.2206 - val_acc: 0.0839\n",
    "<br>Epoch 3/20\n",
    "<br>177/177 [==============================] - 917s 5s/step - loss: 6.1581 - acc: 0.0821 - val_loss: 6.1545 - val_acc: 0.0839\n",
    "<br>Epoch 4/20\n",
    "<br>177/177 [==============================] - 924s 5s/step - loss: 6.1010 - acc: 0.0820 - val_loss: 6.1366 - val_acc: 0.0839\n",
    "<br>Epoch 5/20\n",
    "<br>177/177 [==============================] - 934s 5s/step - loss: 6.0514 - acc: 0.0821 - val_loss: 6.0935 - val_acc: 0.0831\n",
    "<br>Epoch 6/20\n",
    "<br>177/177 [==============================] - 947s 5s/step - loss: 6.0069 - acc: 0.0820 - val_loss: 6.0857 - val_acc: 0.0839\n",
    "<br>Epoch 7/20\n",
    "<br>177/177 [==============================] - 949s 5s/step - loss: 5.9552 - acc: 0.0821 - val_loss: 6.0095 - val_acc: 0.0839\n",
    "<br>Epoch 8/20\n",
    "<br>177/177 [==============================] - 961s 5s/step - loss: 5.8981 - acc: 0.0820 - val_loss: 5.9695 - val_acc: 0.0839\n",
    "<br>Epoch 9/20\n",
    "<br>177/177 [==============================] - 964s 5s/step - loss: 5.8515 - acc: 0.0821 - val_loss: 5.9482 - val_acc: 0.0839\n",
    "<br>Epoch 10/20\n",
    "<br>177/177 [==============================] - 965s 5s/step - loss: 5.8068 - acc: 0.0820 - val_loss: 5.9041 - val_acc: 0.0831\n",
    "<br>Epoch 11/20\n",
    "<br>177/177 [==============================] - 964s 5s/step - loss: 5.7600 - acc: 0.0820 - val_loss: 5.8613 - val_acc: 0.0839\n",
    "\n",
    "### This iteration used the following regularization for both LSTM layers: \n",
    "\n",
    "kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l1(0.01)\n",
    "\n",
    "### Now I'll try the reverse. If that doesn't work I'll use each L1 and L2 on the kernel and activity regularizers and go from there. \n",
    "\n",
    "am2 = LSTM(128, kernel_regularizer=regularizers.l1(0.01),activity_regularizer=regularizers.l2(0.01))(am1)\n",
    "  \n",
    "<br>Epoch 1/20\n",
    "<br>177/177 [==============================] - 779s 4s/step - loss: 18.3084 - acc: 0.0800 - val_loss: 6.0506 - val_acc: 0.0831\n",
    "<br>Epoch 2/20\n",
    "<br>177/177 [==============================] - 826s 5s/step - loss: 5.7435 - acc: 0.0820 - val_loss: 5.8478 - val_acc: 0.0839\n",
    "<br>Epoch 3/20\n",
    "<br>177/177 [==============================] - 851s 5s/step - loss: 5.5926 - acc: 0.0821 - val_loss: 5.7601 - val_acc: 0.0839\n",
    "<br>Epoch 4/20\n",
    "<br>177/177 [==============================] - 863s 5s/step - loss: 5.5332 - acc: 0.0820 - val_loss: 5.7631 - val_acc: 0.0839\n",
    "<br>Epoch 5/20\n",
    "<br>177/177 [==============================] - 870s 5s/step - loss: 5.4987 - acc: 0.0821 - val_loss: 5.7483 - val_acc: 0.0831\n",
    "<br>Epoch 6/20\n",
    "<br>177/177 [==============================] - 873s 5s/step - loss: 5.4781 - acc: 0.0820 - val_loss: 5.7702 - val_acc: 0.0839\n",
    "\n",
    "### Clearly his is also inadequate. Loss started out at 54 and ended epoch 1 at 18.3. Accuracy once again never rose above 8% after eoch 6 so I cancelled the train. \n",
    "\n",
    "### Now I'll try using regularizing just the kernel (another name for the weight). I'll start with L2, then if it shows no progress I'll stop it and try L1. \n",
    "\n",
    "<br>Epoch 1/20\n",
    "<br>177/177 [==============================] - 774s 4s/step - loss: 6.1941 - acc: 0.0809 - val_loss: 5.4953 - val_acc: 0.0831\n",
    "<br>Epoch 2/20\n",
    "<br>177/177 [==============================] - 833s 5s/step - loss: 5.2656 - acc: 0.0811 - val_loss: 5.5533 - val_acc: 0.0839\n",
    "<br>Epoch 3/20\n",
    "<br>177/177 [==============================] - 857s 5s/step - loss: 5.1890 - acc: 0.0821 - val_loss: 5.5148 - val_acc: 0.0839\n",
    "<br> 4/20\n",
    "<br>177/177 [==============================] - 873s 5s/step - loss: 5.1007 - acc: 0.0823 - val_loss: 5.5670 - val_acc: 0.0839\n",
    "<br>Epoch 5/20\n",
    "<br>/177 [========================>.....] - ETA: 2:03 - loss: 5.0226 - acc: 0.0843\n",
    "\n",
    "### Accuracy stays low, both in the training set and the validation. Moving to L1 for both LSTMs:\n",
    "\n",
    "<br>Epoch 1/20\n",
    "<br>177/177 [==============================] - 809s 5s/step - loss: 17.7728 - acc: 0.0808 - val_loss: 5.6576 - val_acc: 0.0831\n",
    "<br>Epoch 2/20\n",
    "<br>177/177 [==============================] - 861s 5s/step - loss: 5.3832 - acc: 0.0820 - val_loss: 5.7030 - val_acc: 0.0839\n",
    "<br>Epoch 3/20\n",
    "<br>177/177 [==============================] - 861s 5s/step - loss: 5.3578 - acc: 0.0821 - val_loss: 5.6998 - val_acc: 0.0839\n",
    "<br>Epoch 4/20\n",
    "<br>14/177 [=>............................] - ETA: 13:29 - loss: 5.2903 - acc: 0.0804\n",
    "\n",
    "### Still not better. \n",
    "\n",
    "### L1 only, Compile optimization to Adadelta (a more robust extension of Adagrad that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients, continuing to learn even when many updates have been done). Other Keras optimizers include adam (default), SGD, Adagrad, Adamax, & Nadam. \n",
    "\n",
    "<br>Epoch 1/20\n",
    "<br>12/177 [=>............................] - ETA: 1:01:57 - loss: 61.0061 - acc: 0.0208\n",
    "\n",
    "### There is no way I can make this config work. The estimated completion time for epoch 1 is over an hour. I'll remove the regularizations completely and cycle through the compile layer optimizers. I also want to try verying my input layer arguments. \n",
    "\n",
    "<br>Epoch 1/20\n",
    "<br>177/177 [==============================] - 750s 4s/step - loss: 5.5397 - acc: 0.0803 - val_loss: 5.3290 - val_acc: 0.0831\n",
    "<br>Epoch 2/20\n",
    "<br>/177 [==============================] - 918s 5s/step - loss: 5.2603 - acc: 0.0809 - val_loss: 5.2948 - val_acc: 0.0855\n",
    "<br>Epoch 3/20\n",
    "<br>177/177 [==============================] - 880s 5s/step - loss: 5.1773 - acc: 0.0931 - val_loss: 5.2079 - val_acc: 0.0995\n",
    "<br>Epoch 4/20\n",
    "<br>177/177 [==============================] - 882s 5s/step - loss: 5.1201 - acc: 0.1013 - val_loss: 5.2185 - val_acc: 0.0987\n",
    "<br>Epoch 5/20\n",
    "<br>177/177 [==============================] - 890s 5s/step - loss: 5.0760 - acc: 0.1024 - val_loss: 5.1989 - val_acc: 0.1028\n",
    "<br>Epoch 6/20\n",
    "<br>177/177 [==============================] - 897s 5s/step - loss: 5.0410 - acc: 0.1028 - val_loss: 5.2196 - val_acc: 0.1012\n",
    "<br>Epoch 7/20\n",
    "<br>177/177 [==============================] - 905s 5s/step - loss: 5.0123 - acc: 0.1037 - val_loss: 5.1821 - val_acc: 0.1036\n",
    "<br>Epoch 8/20\n",
    "<br>76/177 [===========>..................] - ETA: 8:29 - loss: 5.0554 - acc: 0.1032\n",
    " \n",
    "Not a good enough improvement on accuracy, and the loss isn't dropping ike I'd like. \n",
    "\n",
    "### Reincorporating the regularization (L2 only weight & activity)\n",
    "\n",
    "<br>Epoch 1/20\n",
    "<br>177/177 [==============================] - 779s 4s/step - loss: 7.0211 - acc: 0.0795 - val_loss: 6.2430 - val_acc: 0.0831\n",
    "<br>Epoch 2/20\n",
    "<br>177/177 [==============================] - 848s 5s/step - loss: 6.1643 - acc: 0.0820 - val_loss: 6.0637 - val_acc: 0.0839\n",
    "<br>Epoch 3/20\n",
    "<br>177/177 [==============================] - 888s 5s/step - loss: 5.9139 - acc: 0.0825 - val_loss: 5.8035 - val_acc: 0.0839\n",
    "<br>Epoch 4/20\n",
    "<br>177/177 [==============================] - 887s 5s/step - loss: 5.7529 - acc: 0.0820 - val_loss: 5.7301 - val_acc: 0.0839\n",
    "<br>Epoch 5/20\n",
    "<br>177/177 [==============================] - 892s 5s/step - loss: 5.6597 - acc: 0.0821 - val_loss: 5.6648 - val_acc: 0.0831\n",
    "<br>Epoch 6/20\n",
    "<br>177/177 [==============================] - 906s 5s/step - loss: 5.5894 - acc: 0.0820 - val_loss: 5.6367 - val_acc: 0.0839\n",
    "<br>Epoch 7/20\n",
    "<br>177/177 [==============================] - 902s 5s/step - loss: 5.5272 - acc: 0.0821 - val_loss: 5.5537 - val_acc: 0.0839\n",
    "<br>Epoch 8/20\n",
    "<br>177/177 [==============================] - 894s 5s/step - loss: 5.4721 - acc: 0.0822 - val_loss: 5.5172 - val_acc: 0.0839\n",
    "<br>Epoch 9/20\n",
    "<br>177/177 [==============================] - 915s 5s/step - loss: 5.4268 - acc: 0.0828 - val_loss: 5.4964 - val_acc: 0.0839"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Despite using over a dozen variations of RNNs, GloVe embeddings, Seq2Seq modeling, and a variety of hyperparameter adjustments, some of which were left out of this Notebook, over the course of weeks I can see that the RNN is still encountering great difficulty summarizing against large text files like full news articles. While accuracy increases in the overall training set, validation accuracy improves somewhat, but not nearly enough to summarize the entire article. \n",
    "\n",
    "![RNN1 Training Results](recursive-rnn-1-history.png \"RNN1 Training Results\")\n",
    "\n",
    "A potential course of action for future development may be to summarize individual paragraphs or even sentences in articles, combine those summaries into new DataFrames train/test splits and attempt summarizations on the summaries. After all, the point of most news articles is detailed in the first couple sentences followed by increasing detail and context to give the story more depth and meaning. \n",
    "\n",
    "At the time of this writing Google is days away from releasing a bidirectional transformer system, currently known through its AI paper \"Google BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". A bidirectional approach may yield better results than feed-forward designs. \n",
    "\n",
    "Also, a current trend moving away from RNNs entirely is being promoted by Cornell University in the paper [\"Attention Is All You Need\"](https://arvix.org/abs/170603762). This process declares that using a simpler network architecture, one based solely on attention mechanisms. This is worth investigating as well. \n",
    "\n",
    "Text summarization can be used to simplify news articles in ways that semantic analysis does not and potentially be used to create models that compare the objective summaries to the published headlines so to identify sensationalist, \"clickbait\" style journalism. Though this pipeline has not yielded the desired results so far and the time to present as a final capstone has come, I intend to continue working on text summarization as a specialization under the NLP umbrella. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
